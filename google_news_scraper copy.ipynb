{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import transformers\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.4 Safari/605.1.15',\n",
    "    'Content-Type': 'text/html',\n",
    "}\n",
    "\n",
    "fmt = '%m/%d/%Y'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_google_news_scrapper(**params):\n",
    "    \n",
    "    output_file_name = ''\n",
    "    for key, value in params.items():\n",
    "        if key == 'min_date':\n",
    "            min_date = value\n",
    "        if key == 'output_file':\n",
    "            output_file_name = value\n",
    "        if key == 'news':\n",
    "            news_count = value  \n",
    "\n",
    "    params = {\n",
    "    \"q\": \"oil and gas price\",  # search query\n",
    "    \"hl\": \"en\",              # language of the search\n",
    "    \"gl\": \"us\",              # country of the search\n",
    "    \"num\": \"100\",            # number of search results per page\n",
    "    \"tbm\": \"nws\",             # news results\n",
    "    \"tbs\": \"cdr:1,cd_min:{},cd_max:{}\".format(min_date, min_date),  #sort by date    \n",
    "}\n",
    "\n",
    "    response = requests.get(\"https://www.google.com/search\", headers=headers, params=params, timeout=30)\n",
    "    \n",
    "    info = dict()\n",
    "    news_data_dict = dict(url = [], text = [], publish_date = [], find_date =[])\n",
    "    columns = ['url', 'text', 'publish_date', 'find_date']\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    info['date'] = min_date\n",
    "    info['url'] = response.url\n",
    "    info['status_code'] = response.status_code\n",
    "    # print(info)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(\"******** fail ********** \")\n",
    "        return\n",
    "    \n",
    "    # print(response.url)\n",
    "    # news_data_dict['url'] = response.url\n",
    "    # columns.append('url')\n",
    "    \n",
    "    count = 1\n",
    "    \n",
    "    for link in soup.find_all('a'):\n",
    "        link_str = str(link.get('href'))\n",
    "        try:\n",
    "            if link_str.startswith(\"https://\") and link_str.find('google.com') == -1 and link_str.find(\n",
    "                    \"https://www.youtube.com/\") == -1 and link_str.find(\"https://www.blogger.com/\") == -1:\n",
    "                \n",
    "                article = Article(link_str)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                \n",
    "                # print(link_str)\n",
    "                # print(article.authors)\n",
    "                # print(article.publish_date)\n",
    "                # print(article.text)\n",
    "                \n",
    "                news_data_dict['url'].append(link_str)\n",
    "                news_data_dict['text'].append(article.text)\n",
    "                news_data_dict['publish_date'].append(article.publish_date)\n",
    "                news_data_dict['find_date'].append(min_date)\n",
    "\n",
    "                count += 1\n",
    "                \n",
    "                if count >= news_count:\n",
    "                    break\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    master_df = pd.DataFrame(news_data_dict)\n",
    "    \n",
    "    # news_data_df.insert(0, 'index_col', news_data_df.index)\n",
    "    \n",
    "    \"\"\"\n",
    "    if os.path.exists(output_file_name):\n",
    "        keep_header = False\n",
    "    else:\n",
    "        keep_header = True\n",
    "    news_data_df.to_csv(output_file_name, mode='a', header=keep_header)\n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.exists(output_file_name):\n",
    "        \n",
    "        servant_df = master_df\n",
    "        servant_df.to_csv('servant.csv',index=False)\n",
    "        \n",
    "        df = pd.concat(\n",
    "    map(pd.read_csv, [output_file_name, 'servant.csv']), ignore_index=False)\n",
    "        \n",
    "        os.remove('servant.csv')\n",
    "        \n",
    "        df.to_csv(output_file_name,index=False)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        master_df.to_csv(output_file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_news_scrapper(start_date, end_date, news_count, output_file_name):\n",
    "    step_obj = datetime.timedelta(days=1)\n",
    "    start_date_time_obj = datetime.datetime.strptime(start_date, fmt)\n",
    "    end_date_time_obj = datetime.datetime.strptime(end_date, fmt)\n",
    "\n",
    "    while start_date_time_obj <= end_date_time_obj:\n",
    "        start_date = start_date_time_obj.strftime(fmt)\n",
    "        print(start_date)\n",
    "        run_google_news_scrapper(min_date=start_date, max_date=start_date, news=news_count, output_file=output_file_name)\n",
    "        time.sleep(np.random.randint(2, 5))\n",
    "        start_date_time_obj += step_obj\n",
    "\n",
    "### to many unnamed index\n",
    "master_df = pd.read_csv('google_news_data.csv')\n",
    "master_df.reset_index()\n",
    "master_df.to_csv('google_news_data.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_news_report(input_file_name, cleaned_output_file_name, save_index=False):\n",
    "    \n",
    "    df = pd.read_csv(input_file_name)\n",
    "    df = df.sort_index().drop_duplicates(subset=['url'], keep='last')\n",
    "    df = df[['find_date','text']]\n",
    "    df = df.set_index('find_date', drop=True)\n",
    "    df.index = pd.to_datetime(df.index, format=fmt)\n",
    "    df.insert(0, 'text_ID', range(0, len(df)))\n",
    "    df.to_csv(cleaned_output_file_name)\n",
    "    \n",
    "    if save_index:\n",
    "        df_i = pd.DataFrame(df.index)\n",
    "        df_i.to_csv(cleaned_output_file_name[0:-4] + '_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/01/2022\n",
      "10/02/2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Deo/1/envs/py37/lib/python3.8/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname PDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/03/2022\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    news_count = 50 # results per day per page\n",
    "    start_date = '10/01/2022'\n",
    "    # end_date = datetime.datetime.now().strftime(fmt)\n",
    "    end_date = '10/0/2022'\n",
    "    news_raw_filename = 'google_news_data.csv'\n",
    "    \n",
    "    google_news_scrapper(start_date, end_date, news_count, news_raw_filename)\n",
    "    news_cleaned_filename = news_raw_filename[0:-4] + '_cleaned.csv'  \n",
    "    clean_news_report(news_raw_filename, news_cleaned_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6576f3d226de05e644183efd1a42ab37a2df7ccf97f3841010d00ab1ddb3300e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
