{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import transformers\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.4 Safari/605.1.15',\n",
    "    'Content-Type': 'text/html',\n",
    "}\n",
    "fmt = '%m/%d/%Y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_google_news_scrapper(**params):\n",
    "    \n",
    "    for key, value in params.items():\n",
    "        if key == 'min_date':\n",
    "            min_date = value\n",
    "        if key == 'output_file':\n",
    "            output_file = value\n",
    "        if key == 'news':\n",
    "            news = value  \n",
    "\n",
    "    params = {\n",
    "    \"q\": \"oil and gas price\",  # search query\n",
    "    \"hl\": \"en\",              # language of the search\n",
    "    \"gl\": \"us\",              # country of the search\n",
    "    \"num\": \"100\",            # number of search results per page\n",
    "    \"tbm\": \"nws\",             # news results\n",
    "    \"tbs\": \"cdr:1,cd_min:{},cd_max:{}\".format(min_date, min_date),  #sort by date    \n",
    "}\n",
    "\n",
    "    response = requests.get(\"https://www.google.com/search\", headers=headers, params=params, timeout=30)\n",
    "    \n",
    "    news_data_dict = dict(url = [], text = [], publish_date = [], find_date =[])\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(\"******** fail ********** \")\n",
    "        return\n",
    "    \n",
    "    count = 1\n",
    "    \n",
    "    for link in soup.find_all('a'):\n",
    "        link_str = str(link.get('href'))\n",
    "        try:\n",
    "            if link_str.startswith(\"https://\") and link_str.find('google.com') == -1 and link_str.find(\n",
    "                    \"https://www.youtube.com/\") == -1 and link_str.find(\"https://www.blogger.com/\") == -1:\n",
    "                \n",
    "                article = Article(link_str)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                \n",
    "                news_data_dict['url'].append(link_str)\n",
    "                news_data_dict['text'].append(article.text)\n",
    "                news_data_dict['publish_date'].append(article.publish_date)\n",
    "                news_data_dict['find_date'].append(min_date)\n",
    "\n",
    "                count += 1\n",
    "                \n",
    "                if count >= news:\n",
    "                    break\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    master_df = pd.DataFrame(news_data_dict)\n",
    "    \n",
    "    if os.path.exists(output_file):\n",
    "        servant_df = master_df\n",
    "        servant_df.to_csv('servant.csv',index=False)\n",
    "        df = pd.concat(\n",
    "        map(pd.read_csv, [output_file, 'servant.csv']), ignore_index=False)\n",
    "        os.remove('servant.csv')\n",
    "        df.to_csv(output_file,index=False)\n",
    "\n",
    "    else:\n",
    "        master_df.to_csv(output_file,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_news_scrapper(start_date, end_date, news_count, output_file_name):\n",
    "    step_obj = datetime.timedelta(days=1)\n",
    "    start_date_time_obj = datetime.datetime.strptime(start_date, fmt)\n",
    "    end_date_time_obj = datetime.datetime.strptime(end_date, fmt)\n",
    "\n",
    "    while start_date_time_obj <= end_date_time_obj:\n",
    "        start_date = start_date_time_obj.strftime(fmt)\n",
    "        print(start_date)\n",
    "        run_google_news_scrapper(min_date=start_date, max_date=start_date, news=news_count, output_file=output_file_name)\n",
    "        time.sleep(np.random.randint(2, 5))\n",
    "        start_date_time_obj += step_obj\n",
    "\n",
    "    master_df = pd.read_csv(output_file_name)\n",
    "    master_df.reset_index()\n",
    "    master_df.to_csv(output_file_name,index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_news_report(input_file_name, cleaned_output_file_name):\n",
    "    \n",
    "    df = pd.read_csv(input_file_name)\n",
    "    df = df.sort_index().drop_duplicates(subset=['url'], keep='last')\n",
    "    for x in df.index: \n",
    "        if pd.isnull(df.loc[x, 'text']):\n",
    "            df = df.drop(x)\n",
    "    df = df[['find_date','text']]\n",
    "    df = df.set_index('find_date', drop=True)\n",
    "    df.index = pd.to_datetime(df.index, format=fmt)\n",
    "    df.insert(0, 'text_ID', range(0, len(df)))\n",
    "    df.to_csv(cleaned_output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    news_count = 50 # results per day per page\n",
    "    start_date = '10/01/2022'\n",
    "    end_date = '10/21/2022'\n",
    "    news_raw_filename = 'google_news_data.csv'\n",
    "    \n",
    "    google_news_scrapper(start_date, end_date, news_count, news_raw_filename)\n",
    "    news_cleaned_filename = news_raw_filename[0:-4] + '_cleaned.csv'  \n",
    "    clean_news_report(news_raw_filename, news_cleaned_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:49:06) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6576f3d226de05e644183efd1a42ab37a2df7ccf97f3841010d00ab1ddb3300e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
